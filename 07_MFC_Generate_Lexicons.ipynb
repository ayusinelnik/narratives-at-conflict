{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayusinelnik/narratives-at-conflict/blob/main/07_MFC_Generate_Lexicons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO3rL1h5iESz"
      },
      "source": [
        "\n",
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRrPpja7iNOo",
        "outputId": "000cb7f5-e326-4717-fdb4-dae222384a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from nltk import tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG6xf0obrMSr"
      },
      "source": [
        "Combine all issues into one file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFJDZbTuzeZ4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/mfc_v4.0/'\n",
        "\n",
        "\n",
        "files = [f for f in os.listdir(folder_path) if f.endswith('_labeled.json')]\n",
        "\n",
        "merged_data = pd.DataFrame()\n",
        "\n",
        "for file in files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    current_data = pd.read_json(file_path)\n",
        "    print(current_data.info())\n",
        "\n",
        "    merged_data = pd.concat([merged_data, current_data], axis=1)\n",
        "\n",
        "merged_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLRio7GXV54N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37202c0-42ad-4bf3-831f-e9e19b5be79f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# tokenize and remove stopword from the MFC texts\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "\n",
        "def process_text(text):\n",
        "    tokenized_text = tokenize.word_tokenize(text)\n",
        "    return [t for t in tokenized_text if not t.replace(\",\", \"\").replace(\".\",\"\").isdigit() \\\n",
        "                and not t in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uPSbdCbUSVf"
      },
      "outputs": [],
      "source": [
        "# produce counts over frames, articles\n",
        "\n",
        "def do_counts(data):\n",
        "    corpus_counter = Counter()\n",
        "    code_to_counter = defaultdict(Counter)\n",
        "    article_counter = Counter()\n",
        "    article_count = 0\n",
        "\n",
        "    for column in data.columns:\n",
        "        assert \"framing\" in data[column][\"annotations\"]\n",
        "\n",
        "        text = data[column][\"text\"].lower()\n",
        "        article_counter.update(set(tokenize.word_tokenize(text)))\n",
        "        article_count += 1\n",
        "\n",
        "        for annotation_set in data[column][\"annotations\"][\"framing\"]:\n",
        "            corpus_counter.update(process_text(text))\n",
        "\n",
        "            for frame in data[column][\"annotations\"][\"framing\"][annotation_set]:\n",
        "                coded_text = text[int(frame[\"start\"]):int(frame[\"end\"])]\n",
        "                code_to_counter[frame[\"code\"]].update(process_text(coded_text))\n",
        "\n",
        "    return corpus_counter, code_to_counter, article_counter, article_count\n",
        "    # above: words counter over all frames, words counter by frame, all words in all article texts, number of articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAfBhdWuT8MQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def words_to_pmi(background_counter, corpus_counter, code_to_counter, to_return_count = 250):\n",
        "    frame_count = sum([code_to_counter[k] for k in code_to_counter])\n",
        "    background_counter = sum(corpus_counter.values())\n",
        "\n",
        "    word_to_pmi = {}\n",
        "    for word in code_to_counter:\n",
        "        # means it is a partial word or is infrequent\n",
        "        if not word in corpus_counter:\n",
        "            continue\n",
        "\n",
        "        # number of times word appears with this frame\n",
        "        # divide by number of words in frame = p( y | x)\n",
        "        p_y_x = code_to_counter[word] / float(frame_count) # frequency of this words in the frame divided by number of words in the frame\n",
        "\n",
        "        # number of times word appears at all / number of words in corpus = p(y)\n",
        "        p_y = corpus_counter[word] / float(background_counter)\n",
        "\n",
        "        assert (p_y_x > 0 and p_y_x < 1), str(p_y_x) + \" \" +  word\n",
        "        assert (p_y > 0 and p_y < 1), str(p_y) + \" \" +  word\n",
        "\n",
        "        word_to_pmi[word] = math.log(p_y_x / p_y)\n",
        "\n",
        "    return sorted(word_to_pmi, key=word_to_pmi.get, reverse=True)[:to_return_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDiNxXeLd2bW"
      },
      "outputs": [],
      "source": [
        "# execution time approx 10 min\n",
        "\n",
        "corpus_counter, code_to_counter, article_counter, article_count  = do_counts(merged_data)\n",
        "# words counter over all frames, words counter by frame, all words and in how many articles they are, number of articles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWoFKegT4zuk"
      },
      "source": [
        "We discard all words that occur in fewer than 0.5% of documents or in more than 98% of documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7Zacu9E4zLy"
      },
      "outputs": [],
      "source": [
        "# find words to cut (too frequent, too infrenquent) -- this should happen before PMI\n",
        "\n",
        "def get_words_to_cut(article_count, article_counter, min_cutoff=1000, top_cutoff=50): #default cutoffs, will be overriden later\n",
        "\n",
        "    min_num_articles = int(article_count / min_cutoff)\n",
        "    max_num_articles = article_count - int(article_count / top_cutoff)\n",
        "\n",
        "    words_to_cut = [w for w in article_counter if article_counter[w] < min_num_articles or\n",
        "                    article_counter[w] > max_num_articles]\n",
        "    return words_to_cut\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAvLkcwu5dIO"
      },
      "outputs": [],
      "source": [
        "# cut infrequent and frequent words -- update the corpus_counter, that stores all frames words\n",
        "\n",
        "cut_words = get_words_to_cut(article_count, article_counter, 200, 50) # override the defaults\n",
        "\n",
        "corpus_counter = Counter({c:corpus_counter[c] for c in corpus_counter if not c in cut_words})\n",
        "# execution time approx 10 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOzzRLAtUTXj"
      },
      "outputs": [],
      "source": [
        "# calculate PMI\n",
        "import math\n",
        "code_to_lex = {}\n",
        "background_counter = sum(corpus_counter.values())\n",
        "for c in code_to_counter:\n",
        "\n",
        "  if str(c).endswith(('.1', '.2')): #those are the frame codes for main frame and headline frame that we disregard\n",
        "    continue\n",
        "  code_to_lex[c] = words_to_pmi(background_counter, corpus_counter, code_to_counter[c], to_return_count = 250)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXwJzLpUmq5c"
      },
      "outputs": [],
      "source": [
        "#saving F_base\n",
        "\n",
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/code_to_lex.json'\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(code_to_lex, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxuGlzemjno-"
      },
      "source": [
        "# Translate F_base to Russian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkEPifRHlkkG",
        "outputId": "1ca1de9e-cb0a-4358-fd56-b3abf731bf13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VtobybUq19T"
      },
      "outputs": [],
      "source": [
        "from google.cloud import translate_v2 as translate\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def translate_text_google_api(text, target_language='ru'):\n",
        "    try:\n",
        "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\"\n",
        "        client = translate.Client()\n",
        "\n",
        "        result = client.translate(text, target_language=target_language)\n",
        "        translation = result['translatedText']\n",
        "        return text, translation\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error for '{text}': {e}\")\n",
        "        return text, None\n",
        "\n",
        "def translate_structure_google_api(structure):\n",
        "    translated_structure = {}\n",
        "    total_words_translated = 0\n",
        "\n",
        "    for key, word_list in structure.items():\n",
        "        translated_words = []\n",
        "        for word in word_list:\n",
        "            original, translation = translate_text_google_api(word)\n",
        "            translated_words.append(translation)\n",
        "            if translation is not None:\n",
        "                total_words_translated += 1\n",
        "                #print(f\"Word '{word}' translated to '{translation}' successfully. Total words translated: {total_words_translated}\")\n",
        "\n",
        "        translated_structure[key] = translated_words\n",
        "\n",
        "    return translated_structure\n",
        "\n",
        "translated_structure = translate_structure_google_api(code_to_lex)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAjlB9aasrcO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/field_2018/code_to_lex_trans_ru.json'\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(translated_structure, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqy8anQNjLET"
      },
      "source": [
        "# Train embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kktw8rE5k1kk",
        "outputId": "688c81dc-0c67-4b55-ae77-6cdd76c37b1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-37-789314f19d15>:9: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  ru_background_corpus = pd.read_csv(f)\n"
          ]
        }
      ],
      "source": [
        "# change the training corpus to a relevant one when ready\n",
        "\n",
        "import pandas as pd\n",
        "import bz2\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/lenta-ru-news.csv.bz2'\n",
        "\n",
        "with bz2.open(file_path, 'rt', encoding='utf-8') as f: # decompresses the file\n",
        "    ru_background_corpus = pd.read_csv(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5J4I7VLnBPm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "texts = ru_background_corpus['text'].astype(str).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSy8kTUiNz10"
      },
      "outputs": [],
      "source": [
        "# sample as needed\n",
        "texts = texts[:10000] # execution for 100k time approx. 7 min\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
        "\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "tokenized_texts = [\n",
        "    [word for word in text if word.isalnum() and word not in stop_words]\n",
        "    for text in tokenized_texts\n",
        "]\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences=tokenized_texts,\n",
        "    vector_size=200,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=5\n",
        ")\n",
        "\n",
        "model.save(\"/content/drive/MyDrive/Research_Thesis_Bocconi_2023/word2vec_model_ru.model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EorVX3YuA3CB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_article_top_words(input_texts):\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "    c = Counter()\n",
        "    article_counter = Counter()\n",
        "    num_articles = 0\n",
        "\n",
        "    for text in input_texts:\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "        c.update(words)\n",
        "        article_counter.update(set(words))\n",
        "        num_articles += 1\n",
        "\n",
        "    return c, num_articles, article_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwQM_8cAB1nV"
      },
      "outputs": [],
      "source": [
        "#c, num_articles, article_counter = get_article_top_words(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxChzbzGFjJV"
      },
      "outputs": [],
      "source": [
        "\n",
        "top_words, num_articles, article_counter = get_article_top_words(texts)\n",
        "#how many times a word appeared in all texts, # article total, in how many articles has a word appeared\n",
        "\n",
        "vocab = sorted(top_words, key=top_words.get, reverse = True)[:50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtsJEuseKUGV"
      },
      "outputs": [],
      "source": [
        "##### use this to only filter the F_base_translated to the limits of the background corpus vocab\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def seeds_to_real_lex(raw_lex, model, vocab):\n",
        "    wv_model = Word2Vec.load(model)\n",
        "\n",
        "    # Iterate over words in raw_lex\n",
        "    filtered_seeds = [word for word in raw_lex if word in vocab and word in wv_model.wv]\n",
        "\n",
        "    return filtered_seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo9tgklh8opT"
      },
      "outputs": [],
      "source": [
        "model = \"/content/drive/MyDrive/Research_Thesis_Bocconi_2023/word2vec_model_ru.model\"\n",
        "\n",
        "filtered_code_to_lex = {key: seeds_to_real_lex(value, model, vocab) for key, value in code_to_lex.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6dbzBWDnEK2"
      },
      "outputs": [],
      "source": [
        "code_to_lex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T_SKU-_8Dyz"
      },
      "outputs": [],
      "source": [
        "def cluster_seeds(wv, seeds, topn, threshold, num_clusters=1):\n",
        "    X = [wv[s] for s in seeds]\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
        "    expanded_seeds = []\n",
        "    for center in kmeans.cluster_centers_:\n",
        "        expanded_seeds += [x[0] for x in wv.most_similar(positive=[center], topn=topn) if x[1] >= threshold]\n",
        "        #expanded_seeds += [x for x in seeds if (1 - cosine(center, wv[x])) >= threshold]\n",
        "    return set(expanded_seeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hhcp--oLYAS"
      },
      "outputs": [],
      "source": [
        "# final execution\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Research_Thesis_Bocconi_2023/word2vec_model_ru.model\"\n",
        "wv_model = Word2Vec.load(model_path)\n",
        "\n",
        "expanded_seeds_dict = {}\n",
        "\n",
        "for key in filtered_code_to_lex:\n",
        "    expanded_seeds_dict[key] = cluster_seeds(wv_model.wv, filtered_code_to_lex[key], topn=1000, threshold=0.7, num_clusters=1)\n",
        "\n",
        "# Print the number of elements for each key\n",
        "for key, expanded_seeds in expanded_seeds_dict.items():\n",
        "    print(f\"{key}: {len(expanded_seeds)} elements\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc22BB_hi5t4"
      },
      "source": [
        "# Query expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C190mymvRa69"
      },
      "outputs": [],
      "source": [
        "# Again remove the words that appear in 98% and 0,5% of articles\n",
        "\n",
        "cut_words = get_words_to_cut(num_articles, article_counter, 200, 50) # values are adequate for a sample of 10k texts\n",
        "\n",
        "for key, values in expanded_seeds_dict.items():\n",
        "    expanded_seeds_dict[key] = [value for value in values if value not in cut_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpKJs-gkuxZV"
      },
      "outputs": [],
      "source": [
        "#saving F_base_translated_expanded\n",
        "\n",
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/expanded_seeds_dict.json'\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(expanded_seeds_dict, json_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPGJY+0w6DiD7ivrUYVZ7M7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}