{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayusinelnik/narratives-at-conflict/blob/main/06_Keywords_Pairs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X224XtjyeJbq"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjdvtim6fKvJ",
        "outputId": "59ac312f-aa6a-4ce4-b5a0-bae81c652a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS8Onh3Tx9st"
      },
      "source": [
        "# Dataset Read"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# True multiling pairs"
      ],
      "metadata": {
        "id": "lMYYMZVpLeY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this excel is the manually checked/cleaned version\n",
        "\n",
        "test = pd.read_excel('/content/drive/MyDrive/Research_Thesis_Bocconi_2023/multiling_true_pairs.xlsx')"
      ],
      "metadata": {
        "id": "NwEES9rNLjvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the same as above\n",
        "test = pd.read_json('/content/drive/MyDrive/Research_Thesis_Bocconi_2023/train_test/data_test_4_lang.json', lines=True, orient = 'records')"
      ],
      "metadata": {
        "id": "mb6lNlJFrz37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Test from Train"
      ],
      "metadata": {
        "id": "A7CoEbTYaIQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reduces data to train set (= there are no true pairs here)\n",
        "values_to_remove = test['original_article_link'].tolist()\n",
        "data = data[~data['original_article_link'].isin(values_to_remove)]\n"
      ],
      "metadata": {
        "id": "nm9Z5sGddSbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['lang_auto'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk1dfIdqDtx7",
        "outputId": "21df3b5f-d8d5-4d44-c1e8-ceea01cd162e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ru    7786\n",
              "es     659\n",
              "it     460\n",
              "fr     406\n",
              "Name: lang_auto, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Truncate Texts"
      ],
      "metadata": {
        "id": "u3Q4ookWq_me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "\n",
        "def truncate_text(text, max_words=225):\n",
        "    words = text.split()\n",
        "    truncated_text = ' '.join(words[:max_words])\n",
        "    return truncated_text\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "test['truncated_text'] = test['text'].apply(lambda x: truncate_text(x, max_words=225))"
      ],
      "metadata": {
        "id": "wzNJR44Fq69_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Multilingual Pairs for Test Set"
      ],
      "metadata": {
        "id": "s04zuWS9uyG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "all_pairs_df = pd.DataFrame(columns=['original_index', 'text_1', 'lang_1', 'text_2', 'lang_2'])\n",
        "\n",
        "\n",
        "for index in test['original_index'].unique():\n",
        "\n",
        "    subset_df = test[test['original_index'] == index]\n",
        "\n",
        "\n",
        "    for pair in combinations(subset_df.index, 2):\n",
        "        text_1, lang_1 = subset_df.loc[pair[0], ['truncated_text', 'lang_auto']]\n",
        "        text_2, lang_2 = subset_df.loc[pair[1], ['truncated_text', 'lang_auto']]\n",
        "\n",
        "        all_pairs_df = all_pairs_df.append({'original_index': index,\n",
        "                                            'text_1': text_1, 'lang_1': lang_1,\n",
        "                                            'text_2': text_2, 'lang_2': lang_2}, ignore_index=True)\n",
        "\n",
        "# Filter out pairs where the languages are the same\n",
        "test_pairs = all_pairs_df[all_pairs_df['lang_1'] != all_pairs_df['lang_2']]\n"
      ],
      "metadata": {
        "id": "cpTP4bl3u2QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pairs"
      ],
      "metadata": {
        "id": "fS97LWlrvxce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keywords and Embeddings Imports"
      ],
      "metadata": {
        "id": "2V9KrGpVhn1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yake\n",
        "import yake"
      ],
      "metadata": {
        "id": "mN0yOIXxhrmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow-text==2.11.*\"\n",
        "!pip install bokeh\n",
        "!pip install simpleneighbors[annoy]\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "ly6oTG6Mh5Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow-text"
      ],
      "metadata": {
        "id": "rlFE66IRdxFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMHh0MLbs_8K",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Setup common imports and functions\n",
        "\n",
        "import bokeh\n",
        "import bokeh.models\n",
        "import bokeh.plotting\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_text import SentencepieceTokenizer\n",
        "import sklearn.metrics.pairwise\n",
        "\n",
        "from simpleneighbors import SimpleNeighbors\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "\n",
        "def visualize_similarity(embeddings_1, embeddings_2, labels_1, labels_2,\n",
        "                         plot_title,\n",
        "                         x_axis_label='Embeddings 1', y_axis_label='Embeddings 2',\n",
        "                         width=1000, height=600,\n",
        "                         xaxis_font_size='12pt', yaxis_font_size='12pt'):\n",
        "\n",
        "  assert len(embeddings_1) == len(labels_1)\n",
        "  assert len(embeddings_2) == len(labels_2)\n",
        "\n",
        "  # arccos based text similarity (Yang et al. 2019; Cer et al. 2019)\n",
        "  sim = 1 - np.arccos(\n",
        "      sklearn.metrics.pairwise.cosine_similarity(embeddings_1,\n",
        "                                                 embeddings_2))/np.pi\n",
        "\n",
        "  embeddings_1_col, embeddings_2_col, sim_col = [], [], []\n",
        "  for i in range(len(embeddings_1)):\n",
        "    for j in range(len(embeddings_2)):\n",
        "      embeddings_1_col.append(labels_1[i])\n",
        "      embeddings_2_col.append(labels_2[j])\n",
        "      sim_col.append(sim[i][j])\n",
        "  df = pd.DataFrame(zip(embeddings_1_col, embeddings_2_col, sim_col),\n",
        "                    columns=['embeddings_1', 'embeddings_2', 'sim'])\n",
        "\n",
        "  mapper = bokeh.models.LinearColorMapper(\n",
        "      palette=[*reversed(bokeh.palettes.YlOrRd[9])], low=df.sim.min(),\n",
        "      high=df.sim.max())\n",
        "\n",
        "  p = bokeh.plotting.figure(title=plot_title, x_range=labels_1,\n",
        "                            x_axis_location=\"above\",\n",
        "                            y_range=[*reversed(labels_2)],\n",
        "                            width=width, height=height,\n",
        "                            tools=\"save\", toolbar_location='below', tooltips=[\n",
        "                                ('pair', '@embeddings_1 ||| @embeddings_2'),\n",
        "                                ('sim', '@sim{0.2f}')], x_axis_label=x_axis_label, y_axis_label=y_axis_label)\n",
        "\n",
        "  p.rect(x=\"embeddings_1\", y=\"embeddings_2\", width=1, height=1, source=df,\n",
        "         fill_color={'field': 'sim', 'transform': mapper}, line_color=None)\n",
        "\n",
        "  p.title.text_font_size = '12pt'\n",
        "  p.title.align = 'center'\n",
        "  p.title.text_font_style = 'bold'\n",
        "\n",
        "  p.axis.axis_line_color = None\n",
        "  p.axis.major_tick_line_color = None\n",
        "  p.axis.major_label_standoff = 16\n",
        "  p.xaxis.major_label_text_font_size = xaxis_font_size\n",
        "  p.xaxis.major_label_orientation = 0.25 * np.pi\n",
        "  p.yaxis.major_label_text_font_size = yaxis_font_size\n",
        "  p.min_border_right = 300\n",
        "\n",
        "  color_bar = bokeh.models.ColorBar(color_mapper=mapper, location=(0, 0), title='Cosine Similarity')\n",
        "  p.add_layout(color_bar, 'right')\n",
        "\n",
        "  bokeh.io.output_notebook()\n",
        "  bokeh.io.show(p)\n",
        "  return p"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\n",
        "\n",
        "model = hub.load(module_url)\n",
        "\n",
        "def embed_text(input):\n",
        "  return model(input)"
      ],
      "metadata": {
        "id": "7IE6zB_3iADe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keywords Similarity: Produce Results on Hyperparsmeters"
      ],
      "metadata": {
        "id": "mQQ_1-DXGS5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long execution time (about 1h on CPU)"
      ],
      "metadata": {
        "id": "AXLU53zEJsoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "import yake\n",
        "\n",
        "max_ngram_sizes = [1, 3, 5, 10]\n",
        "deduplication_thresholds = [0, 0.2, 0.5, 0.9]\n",
        "numOfKeywords_values = [5, 10, 15, 20]\n",
        "normalized_score_thresholds = [0.2, 0.5, 0.7, 0.9]\n",
        "\n",
        "\n",
        "language1 = \"es\"\n",
        "language2 = \"it\"\n",
        "\n",
        "def calculate_similarity(text1, text2, max_ngram_size, deduplication_threshold, numOfKeywords, normalized_score_threshold):\n",
        "    deduplication_algo = 'seqclst'\n",
        "\n",
        "    kw_extractor1 = yake.KeywordExtractor(lan=language1, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, top=numOfKeywords, n=max_ngram_size, features=None)\n",
        "    keywords1 = kw_extractor1.extract_keywords(text1)\n",
        "\n",
        "    kw_extractor2 = yake.KeywordExtractor(lan=language2, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, top=numOfKeywords, n=max_ngram_size, features=None)\n",
        "    keywords2 = kw_extractor2.extract_keywords(text2)\n",
        "\n",
        "    scores1 = [score for _, score in keywords1]\n",
        "    scaler1 = MinMaxScaler()\n",
        "    normalized_scores1 = 1 - scaler1.fit_transform([[score] for score in scores1])\n",
        "    normalized_scores1 = normalized_scores1.flatten().tolist()\n",
        "    normalized_keyword_scores1 = [(keyword, score, normalized_score) for (keyword, score), normalized_score in zip(keywords1, normalized_scores1)]\n",
        "    filtered_normalized_keyword_scores1 = [(keyword, score, normalized_score) for keyword, score, normalized_score in normalized_keyword_scores1 if normalized_score > normalized_score_threshold]\n",
        "\n",
        "    scores2 = [score for _, score in keywords2]\n",
        "    scaler2 = MinMaxScaler()\n",
        "    normalized_scores2 = 1 - scaler2.fit_transform([[score] for score in scores2])\n",
        "    normalized_scores2 = normalized_scores2.flatten().tolist()\n",
        "    normalized_keyword_scores2 = [(keyword, score, normalized_score) for (keyword, score), normalized_score in zip(keywords2, normalized_scores2)]\n",
        "    filtered_normalized_keyword_scores2 = [(keyword, score, normalized_score) for keyword, score, normalized_score in normalized_keyword_scores2 if normalized_score > normalized_score_threshold]\n",
        "\n",
        "    keyword_list1 = [item[0] for item in filtered_normalized_keyword_scores1]\n",
        "    keyword_list2 = [item[0] for item in filtered_normalized_keyword_scores2]\n",
        "\n",
        "\n",
        "    result1 = embed_text(keyword_list1)\n",
        "    result2 = embed_text(keyword_list2)\n",
        "\n",
        "    # Calculate cosine similarity between the two language results\n",
        "    similarity_matrix = cosine_similarity(result1, result2)\n",
        "    overall_similarity = np.mean(similarity_matrix)\n",
        "\n",
        "    return overall_similarity\n",
        "\n",
        "pair_results = []\n",
        "\n",
        "\n",
        "for max_ngram_size, deduplication_threshold, numOfKeywords, normalized_score_threshold in product(max_ngram_sizes, deduplication_thresholds, numOfKeywords_values, normalized_score_thresholds):\n",
        "    pair_scores = []\n",
        "\n",
        "    for index, row in test_pairs_es.iterrows():\n",
        "        text1, text2 = row['text_1'], row['text_2']\n",
        "        similarity = calculate_similarity(text1, text2, max_ngram_size, deduplication_threshold, numOfKeywords, normalized_score_threshold)\n",
        "        pair_scores.append(similarity)\n",
        "\n",
        "\n",
        "    mean_similarity = np.mean(pair_scores)\n",
        "\n",
        "    pair_results.append({\n",
        "        'max_ngram_size': max_ngram_size,\n",
        "        'deduplication_threshold': deduplication_threshold,\n",
        "        'numOfKeywords': numOfKeywords,\n",
        "        'normalized_score_threshold': normalized_score_threshold,\n",
        "        'mean_similarity': mean_similarity\n",
        "    })\n",
        "\n",
        "pair_results = pd.DataFrame(pair_results)\n",
        "pair_results = pair_results.sort_values(by='mean_similarity', ascending=False)\n"
      ],
      "metadata": {
        "id": "apVyQltJjtIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the best common param combination and each lang pair best params"
      ],
      "metadata": {
        "id": "cOk3M5rnnLsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/'\n",
        "filename_pattern = 'param_results_alltxt_*.json'\n",
        "file_paths = glob.glob(path + filename_pattern)\n",
        "dataframes = []\n",
        "\n",
        "\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_json(file_path, orient='records', lines=True)\n",
        "    dataframes.append(df)\n",
        "\n",
        "\n",
        "for df in dataframes:\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "for df in dataframes:\n",
        "    columns_to_exclude = ['mean_similarity'] #add normalized_score_threshold if needed\n",
        "    df['identifier'] = df.drop(columns=columns_to_exclude).astype(str).agg('-'.join, axis=1)\n",
        "\n",
        "\n",
        "common_identifiers = set(df['identifier'].iloc[0] for df in dataframes[1:]).intersection(dataframes[0]['identifier'])\n",
        "\n",
        "\n",
        "best_rows, highest_mean_similarities = [], []\n",
        "\n",
        "for identifier in common_identifiers:\n",
        "    rows = [df[df['identifier'] == identifier] for df in dataframes]\n",
        "\n",
        "    mean_similarities = [min(row['mean_similarity'].values[0] for row in rows) for row in rows]\n",
        "\n",
        "    if max(mean_similarities) > 0:\n",
        "        highest_mean_similarities.append(max(mean_similarities))\n",
        "        best_rows.append((identifier, rows))\n",
        "    else:\n",
        "        print(f\"Skipping identifier {identifier} as all mean similarities are non-positive.\")\n",
        "        print(f\"Mean Similarities: {mean_similarities}\")\n",
        "\n",
        "\n",
        "sorted_indices = sorted(range(len(highest_mean_similarities)), key=lambda k: highest_mean_similarities[k], reverse=True)\n",
        "\n",
        "\n",
        "for i in range(min(5, len(sorted_indices))):\n",
        "    index = sorted_indices[i]\n",
        "    identifier, rows = best_rows[index]\n",
        "\n",
        "    print(f\"\\nTop {i + 1} Row Identifier: {identifier}\")\n",
        "    print(f\"Highest Mean Similarity: {highest_mean_similarities[index]}\")\n",
        "\n",
        "\n",
        "    for j, row in enumerate(rows):\n",
        "        file_name = file_paths[j].split('/')[-1]  # Extract file name from the path\n",
        "        print(f\"Similarity in {file_name}: {row['mean_similarity'].values[0]}\")\n",
        "        print(f\"Index in {file_name}: {row.index[0]}\")\n"
      ],
      "metadata": {
        "id": "bNlhe638pfEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assign Keywords by Common Best Hyperparameters"
      ],
      "metadata": {
        "id": "jtlz5DClyw-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "import pandas as pd\n",
        "\n",
        "def truncate_text(text, max_words=225):\n",
        "    if isinstance(text, str):\n",
        "        words = text.split()\n",
        "        truncated_text = ' '.join(words[:max_words])\n",
        "        return truncated_text\n",
        "    else:\n",
        "        return str(text)\n",
        "\n",
        "data['truncated_text'] = data['text'].apply(lambda x: truncate_text(x, max_words=225))"
      ],
      "metadata": {
        "id": "BWopYAFYLkGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(subset=['text']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "vWVocszXatBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import yake\n",
        "\n",
        "\n",
        "deduplication_algo = 'seqclst'\n",
        "max_ngram_size = 1\n",
        "deduplication_threshold = 0.5\n",
        "numOfKeywords = 15\n",
        "\n",
        "\n",
        "def extract_keywords_and_normalize(text, language):\n",
        "    kw_extractor = yake.KeywordExtractor(lan=language, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, top=numOfKeywords, n=max_ngram_size, features=None)\n",
        "    keywords = kw_extractor.extract_keywords(text)\n",
        "\n",
        "    if not keywords:\n",
        "      return {}\n",
        "\n",
        "    scores = [score for _, score in keywords]\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "\n",
        "    scores_array = np.array(scores).reshape(-1, 1)\n",
        "\n",
        "    normalized_scores = 1 - scaler.fit_transform(scores_array)\n",
        "    normalized_scores = normalized_scores.flatten().tolist()\n",
        "    keyword_dict = {keyword: normalized_score for (keyword, _), normalized_score in zip(keywords, normalized_scores)}\n",
        "\n",
        "    return keyword_dict\n",
        "\n",
        "data['keywords_auto'] = data.apply(lambda row: extract_keywords_and_normalize(row['truncated_text'], row['lang_auto']), axis=1)\n"
      ],
      "metadata": {
        "id": "WqM1cGRh2rpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "empty_list_mask = data['keywords_auto'].apply(lambda x: len(x) == 0)\n",
        "data = data[~empty_list_mask]\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "7-Nhd9-OGZC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repear for fewer than 5 keywords"
      ],
      "metadata": {
        "id": "AaIgWkTycRee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empty_list_mask = data['keywords_auto'].apply(lambda x: len(x) < 5)\n",
        "data = data[~empty_list_mask]\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "u96cFa7hcQL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filters out Stopwords in Keywords"
      ],
      "metadata": {
        "id": "6Uy_8ASYCiue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "stoplist_files = {\n",
        "    'ru': '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/stop_words/stopwords_ru.txt',\n",
        "    'es': '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/stop_words/stopwords_es.txt',\n",
        "    #'en': '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/stop_words/stopwords_en.txt',\n",
        "    #'de': '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/stop_words/stopwords_de.txt',\n",
        "    'it': '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/stop_words/stopwords_it.txt',\n",
        "    'fr': '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/stop_words/stopwords_fr.txt'\n",
        "}\n",
        "\n",
        "stoplists = {}\n",
        "for lang, stoplist_file in stoplist_files.items():\n",
        "    with open(stoplist_file, 'r', encoding='utf-8') as file:\n",
        "        stoplists[lang] = set(line.strip() for line in file)\n",
        "\n",
        "\n",
        "def remove_stopwords(keywords_dict, language):\n",
        "    if not isinstance(keywords_dict, dict):\n",
        "        return {}\n",
        "\n",
        "    cleaned_keywords_dict = {}\n",
        "    for keyword, score in keywords_dict.items():\n",
        "        if keyword.lower() not in stoplists[language]:\n",
        "            cleaned_keywords_dict[keyword] = score\n",
        "\n",
        "    return cleaned_keywords_dict\n",
        "\n",
        "data['keywords_auto'] = data.apply(lambda row: remove_stopwords(row['keywords_auto'], row['lang_auto']), axis=1)"
      ],
      "metadata": {
        "id": "tlOnuw6ABnjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter Keywords"
      ],
      "metadata": {
        "id": "RV09knCfBWLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out to leave only 5 higest score words each rows"
      ],
      "metadata": {
        "id": "IJD-ayxUDYCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def keep_top_keys(keywords_dict):\n",
        "    if not isinstance(keywords_dict, dict):\n",
        "        return {}\n",
        "\n",
        "    top_5_keywords_dict = {k: keywords_dict[k] for k in list(keywords_dict)[:5]}\n",
        "\n",
        "    return top_5_keywords_dict\n",
        "\n",
        "data['keywords_auto'] = data['keywords_auto'].apply(keep_top_keys)"
      ],
      "metadata": {
        "id": "SXA2sWNNEG-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Pairs by New Keywords"
      ],
      "metadata": {
        "id": "E7q1JaUCBOTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the dicts into lists, remove the scores"
      ],
      "metadata": {
        "id": "B4aiWWVRFOqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Pairs"
      ],
      "metadata": {
        "id": "DWVnAHnVJSO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick fix if the dates appear in strange long numbers (miliseconds)\n",
        "\n",
        "import datetime\n",
        "\n",
        "def convert_timestamp_to_date(timestamp):\n",
        "    return datetime.datetime.utcfromtimestamp(timestamp / 1000).strftime('%d/%m/%Y')\n",
        "\n",
        "data['Date_Of_Publication'] = data['Date_Of_Publication'].apply(convert_timestamp_to_date)\n"
      ],
      "metadata": {
        "id": "rA4UOQ46kfgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick manual fix for dates == 0. The keywords and other html columns are empty too, should come back and check/fix\n",
        "\n",
        "row_with_numeric_zero = data[data['Date_Of_Publication'] == 0]\n",
        "data['Date_Of_Publication'] = data['Date_Of_Publication'].astype(str)\n",
        "data = data.replace({'Date_Of_Publication': {'0': '10/07/2020'}})\n"
      ],
      "metadata": {
        "id": "DdIFyTilkh3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import datetime\n",
        "\n",
        "data['Date_Of_Publication'] = pd.to_datetime(data['Date_Of_Publication'], format='%d/%m/%Y')\n",
        "\n",
        "\n",
        "time_window = datetime.timedelta(weeks=4) # +- 4 weeks\n",
        "\n",
        "\n",
        "result_df = pd.DataFrame(columns=['ru_text', 'fr_text', 'ru_keywords', 'fr_keywords', 'similarity'])\n",
        "\n",
        "# the code loops through the french articles\n",
        "\n",
        "for fr_index, fr_row in data[data['lang_auto'] == 'fr'].iterrows(): # Check the language ###################\n",
        "    fr_date = fr_row['Date_Of_Publication']\n",
        "    fr_keywords = fr_row['keywords_auto']\n",
        "\n",
        "    # check russian matches\n",
        "    ru_candidates = data[(data['lang_auto'] == 'ru') & (data['Date_Of_Publication'] >= fr_date - time_window) & (data['Date_Of_Publication'] <= fr_date + time_window)]\n",
        "\n",
        "\n",
        "    if not ru_candidates.empty:\n",
        "        best_similarity = -1\n",
        "        best_ru_row = None\n",
        "\n",
        "        for ru_index, ru_row in ru_candidates.iterrows():\n",
        "            ru_keywords = ru_row['keywords_auto']\n",
        "\n",
        "\n",
        "            fr_result = embed_text(fr_keywords) # The algorithm from MUSE\n",
        "            ru_result = embed_text(ru_keywords)\n",
        "            similarity_matrix = cosine_similarity(fr_result, ru_result)\n",
        "            overall_similarity = np.mean(similarity_matrix)\n",
        "\n",
        "\n",
        "            if overall_similarity > best_similarity: # Update best similarity\n",
        "                best_similarity = overall_similarity\n",
        "                best_ru_row = ru_row\n",
        "\n",
        "\n",
        "        if best_ru_row is not None:\n",
        "            result_df = result_df.append({\n",
        "                'ru_text': best_ru_row['text'],\n",
        "                'fr_text': fr_row['text'],\n",
        "                'ru_keywords': best_ru_row['keywords_auto'],\n",
        "                'fr_keywords': fr_keywords,\n",
        "                'similarity': best_similarity\n",
        "            }, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "xrp81GqaCKuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize one Example of Keyword Matching"
      ],
      "metadata": {
        "id": "4A8yJtCeQ2yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "deduplication_algo = 'seqclst'\n",
        "max_ngram_size = 1\n",
        "deduplication_threshold = 0.9\n",
        "numOfKeywords = 15\n",
        "language = \"fr\"\n",
        "\n",
        "text_sp = data['fr_truncated_text'][57]\n",
        "\n",
        "kw_extractor = yake.KeywordExtractor(lan = language, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, top=numOfKeywords,n=max_ngram_size, features=None) #\n",
        "\n",
        "keywords_1 = kw_extractor.extract_keywords(text_sp)\n",
        "\n",
        "scores = [score for _, score in keywords_1]\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "normalized_scores = 1 - scaler.fit_transform([[score] for score in scores])\n",
        "normalized_scores = normalized_scores.flatten().tolist()\n",
        "normalized_keyword_scores_1 = [(keyword, score, normalized_score) for (keyword, score), normalized_score in zip(keywords_1, normalized_scores)]\n",
        "filtered_normalized_keyword_scores_1 = [(keyword, score, normalized_score) for keyword, score, normalized_score in normalized_keyword_scores_1 if normalized_score > 0.0]\n",
        "\n",
        "\n",
        "deduplication_algo = 'seqclst'\n",
        "max_ngram_size = 1\n",
        "deduplication_threshold = 0.9\n",
        "numOfKeywords = 15\n",
        "language = \"es\"\n",
        "\n",
        "text_ru = data['es_truncated_text'][57]\n",
        "\n",
        "kw_extractor = yake.KeywordExtractor(lan = language, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, top=numOfKeywords, n=max_ngram_size, features=None) #\n",
        "keywords_2 = kw_extractor.extract_keywords(text_ru)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scores = [score for _, score in keywords_2]\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "normalized_scores = 1 - scaler.fit_transform([[score] for score in scores])\n",
        "normalized_scores = normalized_scores.flatten().tolist()\n",
        "normalized_keyword_scores_2 = [(keyword, score, normalized_score) for (keyword, score), normalized_score in zip(keywords_2, normalized_scores)]\n",
        "filtered_normalized_keyword_scores_2 = [(keyword, score, normalized_score) for keyword, score, normalized_score in normalized_keyword_scores_2 if normalized_score > 0.0]\n",
        "\n",
        "\n",
        "filtered_normalized_keyword_scores_1\n",
        "filtered_normalized_keyword_scores_2\n",
        "\n",
        "spanish_keywords = [item[0] for item in filtered_normalized_keyword_scores_1[:5]]\n",
        "russian_keywords = [item[0] for item in filtered_normalized_keyword_scores_2[:5]]\n",
        "\n",
        "sp_result = embed_text(spanish_keywords)\n",
        "ru_result = embed_text(russian_keywords)\n",
        "\n",
        "similarity_matrix = cosine_similarity(sp_result, ru_result)\n",
        "overall_similarity = np.mean(similarity_matrix)\n",
        "\n",
        "#print(f\"Overall Similarity: {overall_similarity}\")"
      ],
      "metadata": {
        "id": "nI-r4fvtQ6NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure = visualize_similarity(sp_result, ru_result, spanish_keywords, russian_keywords, 'French to Spanish Keyword Similarity Matrix', x_axis_label='French Keywords', y_axis_label='Spanish Keywords', width=800, height=400,\n",
        "                         xaxis_font_size='10pt', yaxis_font_size='10pt')"
      ],
      "metadata": {
        "id": "yK45xAilQ-nI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "d54231cd-9402-4bb6-cd03-4ad51222691d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  const force = true;\n",
              "\n",
              "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "const JS_MIME_TYPE = 'application/javascript';\n",
              "  const HTML_MIME_TYPE = 'text/html';\n",
              "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
              "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
              "\n",
              "  /**\n",
              "   * Render data to the DOM node\n",
              "   */\n",
              "  function render(props, node) {\n",
              "    const script = document.createElement(\"script\");\n",
              "    node.appendChild(script);\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when an output is cleared or removed\n",
              "   */\n",
              "  function handleClearOutput(event, handle) {\n",
              "    function drop(id) {\n",
              "      const view = Bokeh.index.get_by_id(id)\n",
              "      if (view != null) {\n",
              "        view.model.document.clear()\n",
              "        Bokeh.index.delete(view)\n",
              "      }\n",
              "    }\n",
              "\n",
              "    const cell = handle.cell;\n",
              "\n",
              "    const id = cell.output_area._bokeh_element_id;\n",
              "    const server_id = cell.output_area._bokeh_server_id;\n",
              "\n",
              "    // Clean up Bokeh references\n",
              "    if (id != null) {\n",
              "      drop(id)\n",
              "    }\n",
              "\n",
              "    if (server_id !== undefined) {\n",
              "      // Clean up Bokeh references\n",
              "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
              "      cell.notebook.kernel.execute(cmd_clean, {\n",
              "        iopub: {\n",
              "          output: function(msg) {\n",
              "            const id = msg.content.text.trim()\n",
              "            drop(id)\n",
              "          }\n",
              "        }\n",
              "      });\n",
              "      // Destroy server and session\n",
              "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
              "      cell.notebook.kernel.execute(cmd_destroy);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when a new output is added\n",
              "   */\n",
              "  function handleAddOutput(event, handle) {\n",
              "    const output_area = handle.output_area;\n",
              "    const output = handle.output;\n",
              "\n",
              "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
              "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
              "      return\n",
              "    }\n",
              "\n",
              "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
              "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
              "      // store reference to embed id on output_area\n",
              "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "    }\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "      const bk_div = document.createElement(\"div\");\n",
              "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "      const script_attrs = bk_div.children[0].attributes;\n",
              "      for (let i = 0; i < script_attrs.length; i++) {\n",
              "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
              "      }\n",
              "      // store reference to server id on output_area\n",
              "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function register_renderer(events, OutputArea) {\n",
              "\n",
              "    function append_mime(data, metadata, element) {\n",
              "      // create a DOM node to render to\n",
              "      const toinsert = this.create_output_subarea(\n",
              "        metadata,\n",
              "        CLASS_NAME,\n",
              "        EXEC_MIME_TYPE\n",
              "      );\n",
              "      this.keyboard_manager.register_events(toinsert);\n",
              "      // Render to node\n",
              "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "      render(props, toinsert[toinsert.length - 1]);\n",
              "      element.append(toinsert);\n",
              "      return toinsert\n",
              "    }\n",
              "\n",
              "    /* Handle when an output is cleared or removed */\n",
              "    events.on('clear_output.CodeCell', handleClearOutput);\n",
              "    events.on('delete.Cell', handleClearOutput);\n",
              "\n",
              "    /* Handle when a new output is added */\n",
              "    events.on('output_added.OutputArea', handleAddOutput);\n",
              "\n",
              "    /**\n",
              "     * Register the mime type and append_mime function with output_area\n",
              "     */\n",
              "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "      /* Is output safe? */\n",
              "      safe: true,\n",
              "      /* Index of renderer in `output_area.display_order` */\n",
              "      index: 0\n",
              "    });\n",
              "  }\n",
              "\n",
              "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
              "  if (root.Jupyter !== undefined) {\n",
              "    const events = require('base/js/events');\n",
              "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  }\n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
              "     \"<div style='background-color: #fdd'>\\n\"+\n",
              "     \"<p>\\n\"+\n",
              "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
              "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
              "     \"</p>\\n\"+\n",
              "     \"<ul>\\n\"+\n",
              "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
              "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
              "     \"</ul>\\n\"+\n",
              "     \"<code>\\n\"+\n",
              "     \"from bokeh.resources import INLINE\\n\"+\n",
              "     \"output_notebook(resources=INLINE)\\n\"+\n",
              "     \"</code>\\n\"+\n",
              "     \"</div>\"}};\n",
              "\n",
              "  function display_loaded() {\n",
              "    const el = document.getElementById(null);\n",
              "    if (el != null) {\n",
              "      el.textContent = \"BokehJS is loading...\";\n",
              "    }\n",
              "    if (root.Bokeh !== undefined) {\n",
              "      if (el != null) {\n",
              "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(display_loaded, 100)\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls == null || js_urls.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "\n",
              "    function on_error(url) {\n",
              "      console.error(\"failed to load \" + url);\n",
              "    }\n",
              "\n",
              "    for (let i = 0; i < css_urls.length; i++) {\n",
              "      const url = css_urls[i];\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error.bind(null, url);\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }\n",
              "\n",
              "    for (let i = 0; i < js_urls.length; i++) {\n",
              "      const url = js_urls[i];\n",
              "      const element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error.bind(null, url);\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n",
              "  const css_urls = [];\n",
              "\n",
              "  const inline_js = [    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "function(Bokeh) {\n",
              "    }\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    if (root.Bokeh !== undefined || force === true) {\n",
              "          for (let i = 0; i < inline_js.length; i++) {\n",
              "      inline_js[i].call(root, root.Bokeh);\n",
              "    }\n",
              "} else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    } else if (force !== true) {\n",
              "      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n",
              "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
              "    }\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(css_urls, js_urls, function() {\n",
              "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(null);\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"d02735e8-1018-4b81-9e0c-f7b63ab9c0cc\" data-root-id=\"p2251\" style=\"display: contents;\"></div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function embed_document(root) {\n",
              "  const docs_json = {\"e359b3eb-a470-4448-aa34-90a51ca22d16\":{\"version\":\"3.3.4\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p2251\",\"attributes\":{\"width\":800,\"height\":400,\"x_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p2261\",\"attributes\":{\"factors\":[\"Kremlin\",\"Russie\",\"l\\u2019Ukraine\",\"Reprochant\",\"l\\u2019Occident\"]}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p2262\",\"attributes\":{\"factors\":[\"Putin\",\"Occidente\",\"guerra\",\"Ucrania\",\"Rusia\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p2263\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p2264\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p2254\",\"attributes\":{\"text\":\"French to Spanish Keyword Similarity Matrix\",\"text_font_size\":\"12pt\",\"align\":\"center\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p2286\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p2277\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p2278\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p2279\"},\"data\":{\"type\":\"map\",\"entries\":[[\"index\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAEAAAACAAAAAwAAAAQAAAAFAAAABgAAAAcAAAAIAAAACQAAAAoAAAALAAAADAAAAA0AAAAOAAAADwAAABAAAAARAAAAEgAAABMAAAAUAAAAFQAAABYAAAAXAAAAGAAAAA==\"},\"shape\":[25],\"dtype\":\"int32\",\"order\":\"little\"}],[\"embeddings_1\",{\"type\":\"ndarray\",\"array\":[\"Kremlin\",\"Kremlin\",\"Kremlin\",\"Kremlin\",\"Kremlin\",\"Russie\",\"Russie\",\"Russie\",\"Russie\",\"Russie\",\"l\\u2019Ukraine\",\"l\\u2019Ukraine\",\"l\\u2019Ukraine\",\"l\\u2019Ukraine\",\"l\\u2019Ukraine\",\"Reprochant\",\"Reprochant\",\"Reprochant\",\"Reprochant\",\"Reprochant\",\"l\\u2019Occident\",\"l\\u2019Occident\",\"l\\u2019Occident\",\"l\\u2019Occident\",\"l\\u2019Occident\"],\"shape\":[25],\"dtype\":\"object\",\"order\":\"little\"}],[\"embeddings_2\",{\"type\":\"ndarray\",\"array\":[\"Rusia\",\"Ucrania\",\"guerra\",\"Occidente\",\"Putin\",\"Rusia\",\"Ucrania\",\"guerra\",\"Occidente\",\"Putin\",\"Rusia\",\"Ucrania\",\"guerra\",\"Occidente\",\"Putin\",\"Rusia\",\"Ucrania\",\"guerra\",\"Occidente\",\"Putin\",\"Rusia\",\"Ucrania\",\"guerra\",\"Occidente\",\"Putin\"],\"shape\":[25],\"dtype\":\"object\",\"order\":\"little\"}],[\"sim\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"K8QsP8LMKj9s7Bo/1mEZP5zZMD+6smE/rn8+P1YUIz+6WyY/BJg6P/SmNj+selU/TBYkP956Jj9SMTA/vY8QPwzoDD/M7Rc/KVMRP1cYFD/yyCg/SGAmP4MIIT/lp2Q/uIQiPw==\"},\"shape\":[25],\"dtype\":\"float32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p2287\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p2288\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p2283\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"embeddings_1\"},\"y\":{\"type\":\"field\",\"field\":\"embeddings_2\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":null},\"fill_color\":{\"type\":\"field\",\"field\":\"sim\",\"transform\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p2250\",\"attributes\":{\"palette\":[\"#ffffcc\",\"#ffeda0\",\"#fed976\",\"#feb24c\",\"#fd8d3c\",\"#fc4e2a\",\"#e31a1c\",\"#bd0026\",\"#800026\"],\"low\":0.5504157543182373,\"high\":0.8931868672370911}}}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p2284\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"embeddings_1\"},\"y\":{\"type\":\"field\",\"field\":\"embeddings_2\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":null},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"field\",\"field\":\"sim\",\"transform\":{\"id\":\"p2250\"}},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"p2285\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"embeddings_1\"},\"y\":{\"type\":\"field\",\"field\":\"embeddings_2\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":null},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"field\",\"field\":\"sim\",\"transform\":{\"id\":\"p2250\"}},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p2260\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p2275\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p2276\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"pair\",\"@embeddings_1 ||| @embeddings_2\"],[\"sim\",\"@sim{0.2f}\"]]}}]}},\"toolbar_location\":\"below\",\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p2270\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p2271\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p2272\"},\"axis_label\":\"Spanish Keywords\",\"major_label_standoff\":16,\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p2273\"},\"major_label_text_font_size\":\"10pt\",\"axis_line_color\":null,\"major_tick_line_color\":null}}],\"right\":[{\"type\":\"object\",\"name\":\"ColorBar\",\"id\":\"p2289\",\"attributes\":{\"location\":[0,0],\"title\":\"Cosine Similarity\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"NoOverlap\",\"id\":\"p2290\"},\"color_mapper\":{\"id\":\"p2250\"}}}],\"above\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p2265\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p2266\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p2267\"},\"axis_label\":\"French Keywords\",\"major_label_standoff\":16,\"major_label_orientation\":0.7853981633974483,\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p2268\"},\"major_label_text_font_size\":\"10pt\",\"axis_line_color\":null,\"major_tick_line_color\":null}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p2269\",\"attributes\":{\"axis\":{\"id\":\"p2265\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p2274\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p2270\"}}}],\"min_border_right\":300}}]}};\n",
              "  const render_items = [{\"docid\":\"e359b3eb-a470-4448-aa34-90a51ca22d16\",\"roots\":{\"p2251\":\"d02735e8-1018-4b81-9e0c-f7b63ab9c0cc\"},\"root_ids\":[\"p2251\"]}];\n",
              "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "  }\n",
              "  if (root.Bokeh !== undefined) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    let attempts = 0;\n",
              "    const timer = setInterval(function(root) {\n",
              "      if (root.Bokeh !== undefined) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else {\n",
              "        attempts++;\n",
              "        if (attempts > 100) {\n",
              "          clearInterval(timer);\n",
              "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
              "        }\n",
              "      }\n",
              "    }, 10, root)\n",
              "  }\n",
              "})(window);"
            ],
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "p2251"
            }
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}