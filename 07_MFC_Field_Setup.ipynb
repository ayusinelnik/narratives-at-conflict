{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayusinelnik/narratives-at-conflict/blob/main/07_MFC_Field_Setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO3rL1h5iESz"
      },
      "source": [
        "\n",
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRrPpja7iNOo",
        "outputId": "000cb7f5-e326-4717-fdb4-dae222384a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from nltk import tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYhBEOgpnFGy"
      },
      "outputs": [],
      "source": [
        "# the info with annotation is available in the _labled files, no need to open the ones with duplicates\n",
        "#text = pd.read_json('/content/drive/MyDrive/Research_Thesis_Bocconi_2023/mfc_v4.0/climate_all_with_duplicates.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmfolhCcOH71"
      },
      "outputs": [],
      "source": [
        "# to read each individual labeed file\n",
        "#data = pd.read_json('/content/drive/MyDrive/Research_Thesis_Bocconi_2023/mfc_v4.0/climate_labeled.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tHqfgIcMEcl"
      },
      "outputs": [],
      "source": [
        "# File that maps frame codes into explicit frame names\n",
        "\n",
        "# with open('/content/drive/MyDrive/Research_Thesis_Bocconi_2023/mfc_v4.0/codes.json', 'r') as file:\n",
        "#   json_content = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG6xf0obrMSr"
      },
      "source": [
        "Combine all issues into one file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFJDZbTuzeZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fec36e9-5b96-4946-8409-1cb23edfaa4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10 entries, annotations to source\n",
            "Columns: 5155 entries, climate_change1.0-1 to climate_change1.0-9994\n",
            "dtypes: object(5155)\n",
            "memory usage: 402.8+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10 entries, annotations to source\n",
            "Columns: 6398 entries, death_penalty_100 to death_penalty_9999\n",
            "dtypes: object(6398)\n",
            "memory usage: 499.9+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10 entries, annotations to source\n",
            "Columns: 10383 entries, gun_control1.0-10 to gun_control1.0-9992\n",
            "dtypes: object(10383)\n",
            "memory usage: 811.2+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10 entries, annotations to source\n",
            "Columns: 6757 entries, Immigration1.0-10005 to Immigration1.0-9998\n",
            "dtypes: object(6757)\n",
            "memory usage: 528.0+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10 entries, annotations to source\n",
            "Columns: 10583 entries, same-sex_marriage1.0-10 to same-sex_marriage1.0-9999\n",
            "dtypes: object(10583)\n",
            "memory usage: 826.9+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10 entries, annotations to source\n",
            "Columns: 5274 entries, Tobacco1.0-100 to Tobacco1.0-9998\n",
            "dtypes: object(5274)\n",
            "memory usage: 412.1+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10 entries, annotations to source\n",
            "Columns: 44550 entries, climate_change1.0-1 to Tobacco1.0-9998\n",
            "dtypes: object(44550)\n",
            "memory usage: 3.4+ MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/mfc_v4.0/'\n",
        "\n",
        "# Get a list of all files in the folder that end with '_labeled.json'\n",
        "files = [f for f in os.listdir(folder_path) if f.endswith('_labeled.json')]\n",
        "\n",
        "merged_data = pd.DataFrame()\n",
        "\n",
        "# Merge data\n",
        "for file in files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    current_data = pd.read_json(file_path)\n",
        "    print(current_data.info())\n",
        "\n",
        "\n",
        "    merged_data = pd.concat([merged_data, current_data], axis=1)\n",
        "\n",
        "merged_data.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N-yv7l-efaO"
      },
      "source": [
        "Map the text spans into actual text chunks -- my code improvisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvxKnXqrQLXY"
      },
      "outputs": [],
      "source": [
        "# my code\n",
        "#import pandas as pd\n",
        "\n",
        "#def extract_framing_text(data):\n",
        "#    for column in data.columns:\n",
        "#        if \"framing\" in data[column][\"annotations\"]:\n",
        "#            for annotator, annotations in data[column][\"annotations\"][\"framing\"].items():\n",
        "#                print(f\"Annotations for column {column} by {annotator}:\")\n",
        "#                for annotation in annotations:\n",
        "#                    start = annotation[\"start\"]\n",
        "#                    end = annotation[\"end\"]\n",
        "#                    code = annotation[\"code\"]\n",
        "#                    extracted_text = data[column][\"text\"][start:end]\n",
        "#                    print(f\"Extracted Text (Code {code}): {extracted_text}\\n\")\n",
        "#        else:\n",
        "#            print(f\"No framing annotations found in column {column}.\\n\")\n",
        "\n",
        "\n",
        "#extract_framing_text(data.iloc[:,1:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_QBbupbiF30"
      },
      "source": [
        "# Approach of Field, 2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLRio7GXV54N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37202c0-42ad-4bf3-831f-e9e19b5be79f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# tokenize and remove stopword from the MFC texts\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "\n",
        "def process_text(text):\n",
        "    tokenized_text = tokenize.word_tokenize(text)\n",
        "    return [t for t in tokenized_text if not t.replace(\",\", \"\").replace(\".\",\"\").isdigit() \\\n",
        "                and not t in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uPSbdCbUSVf"
      },
      "outputs": [],
      "source": [
        "# her code adapted by me to produce counts over frames, articles\n",
        "\n",
        "def do_counts(data):\n",
        "    corpus_counter = Counter()\n",
        "    code_to_counter = defaultdict(Counter)\n",
        "    article_counter = Counter()\n",
        "    article_count = 0\n",
        "\n",
        "    for column in data.columns:\n",
        "        assert \"framing\" in data[column][\"annotations\"]\n",
        "\n",
        "        text = data[column][\"text\"].lower()\n",
        "        article_counter.update(set(tokenize.word_tokenize(text)))\n",
        "        article_count += 1\n",
        "\n",
        "        for annotation_set in data[column][\"annotations\"][\"framing\"]:\n",
        "            corpus_counter.update(process_text(text))\n",
        "\n",
        "            for frame in data[column][\"annotations\"][\"framing\"][annotation_set]:\n",
        "                coded_text = text[int(frame[\"start\"]):int(frame[\"end\"])]\n",
        "                code_to_counter[frame[\"code\"]].update(process_text(coded_text))\n",
        "\n",
        "    return corpus_counter, code_to_counter, article_counter, article_count\n",
        "    # above: words counter over all frames, words counter by frame, all words in all article texts, number of articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAfBhdWuT8MQ"
      },
      "outputs": [],
      "source": [
        "# her code\n",
        "# the result will have to go over frames and return results per frame -- we do later\n",
        "\n",
        "def words_to_pmi(background_counter, corpus_counter, code_to_counter, to_return_count = 250):\n",
        "    frame_count = sum([code_to_counter[k] for k in code_to_counter])\n",
        "    background_counter = sum(corpus_counter.values())\n",
        "\n",
        "    word_to_pmi = {}\n",
        "    for word in code_to_counter:\n",
        "        # means it is a partial word or is infrequent\n",
        "        if not word in corpus_counter:\n",
        "            continue\n",
        "\n",
        "        # number of times word appears with this frame\n",
        "        # divide by number of words in frame = p( y | x)\n",
        "        p_y_x = code_to_counter[word] / float(frame_count) # frequency of this words in the frame divided by number of words in the frame\n",
        "\n",
        "        # number of times word appears at all / number of words in corpus = p(y)\n",
        "        p_y = corpus_counter[word] / float(background_counter)\n",
        "\n",
        "        assert (p_y_x > 0 and p_y_x < 1), str(p_y_x) + \" \" +  word\n",
        "        assert (p_y > 0 and p_y < 1), str(p_y) + \" \" +  word\n",
        "\n",
        "        word_to_pmi[word] = math.log(p_y_x / p_y)\n",
        "\n",
        "    return sorted(word_to_pmi, key=word_to_pmi.get, reverse=True)[:to_return_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDiNxXeLd2bW"
      },
      "outputs": [],
      "source": [
        "#merged_data  = merged_data.iloc[:, 1:3]\n",
        "\n",
        "# words counter over all frames, words counter by frame, all words and in how many articles they are, number of articles\n",
        "\n",
        "corpus_counter, code_to_counter, article_counter, article_count  = do_counts(merged_data)\n",
        "# execution time approx 10 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIOHHWqdVCtw"
      },
      "outputs": [],
      "source": [
        "corpus_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMxutOO7SiPR",
        "outputId": "5c80f85c-f6aa-4fe2-a00c-aff7bb38cb62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3784"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(corpus_counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWoFKegT4zuk"
      },
      "source": [
        "We discard all words that occur in fewer than 0.5% of documents or in more than 98% of documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7Zacu9E4zLy"
      },
      "outputs": [],
      "source": [
        "# find words to cut (too frequent, too infrenquent) -- this should happen before PMI\n",
        "\n",
        "def get_words_to_cut(article_count, article_counter, min_cutoff=1000, top_cutoff=50): #default cutoffs, will be overriden later\n",
        "\n",
        "    min_num_articles = int(article_count / min_cutoff)\n",
        "    max_num_articles = article_count - int(article_count / top_cutoff)\n",
        "\n",
        "    words_to_cut = [w for w in article_counter if article_counter[w] < min_num_articles or\n",
        "                    article_counter[w] > max_num_articles]\n",
        "    return words_to_cut\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAvLkcwu5dIO"
      },
      "outputs": [],
      "source": [
        "# cut infrequent and frequent words -- update the corpus_counter, that stores all frames words\n",
        "# she originally had 1000 and 50, which i substituted to keep the percentsges as in the paper, 0.5% and 98% of articles respectively\n",
        "\n",
        "cut_words = get_words_to_cut(article_count, article_counter, 200, 50) # override the defaults\n",
        "\n",
        "corpus_counter = Counter({c:corpus_counter[c] for c in corpus_counter if not c in cut_words})\n",
        "# execution time approx 10 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIjEaAYXR9IO"
      },
      "outputs": [],
      "source": [
        "corpus_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOzzRLAtUTXj"
      },
      "outputs": [],
      "source": [
        "# calculate PMI\n",
        "import math\n",
        "code_to_lex = {}\n",
        "background_counter = sum(corpus_counter.values())\n",
        "for c in code_to_counter:\n",
        "\n",
        "  if str(c).endswith(('.1', '.2')): #those are the frame codes for main frame and headline frame that we disregard\n",
        "  #if \"primary\" in code_to_str[c] or \"headline\" in code_to_str[c] or \"primany\" in code_to_str[c]: #should map frame codes into names\n",
        "    continue\n",
        "  code_to_lex[c] = words_to_pmi(background_counter, corpus_counter, code_to_counter[c], to_return_count = 250)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXwJzLpUmq5c"
      },
      "outputs": [],
      "source": [
        "#saving F_base\n",
        "\n",
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/code_to_lex.json'\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(code_to_lex, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROlqdbQWm-uH"
      },
      "source": [
        "When our test corpus is in a different language (i.e. Russian), we use Google Translate to translate Fbase into the new language. We restrict our vocabulary to the 50,000 most frequent words in the test corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6QN2qFDoHeG"
      },
      "source": [
        "Then, to perform the query-expansion, we train 200-dimensional word embeddings on a large background corpus in the test language, using CBOW with a 5-word context window (Mikolov et al., 2013). We compute the center of each lexicon, c, by summing the embeddings for all words in the lexicon. We then identify up to the K nearest neighbors to this center, determined by the cosine distance from c, as long as the cosine distance is not greater than a manually-chosen threshold (t).3 We again filtered the final set by removing all words that occur in fewer than 0.5% of documents or in more than 98% of documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxuGlzemjno-"
      },
      "source": [
        "# Translate F_base RU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkEPifRHlkkG",
        "outputId": "1ca1de9e-cb0a-4358-fd56-b3abf731bf13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VtobybUq19T"
      },
      "outputs": [],
      "source": [
        "from google.cloud import translate_v2 as translate\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def translate_text_google_api(text, target_language='ru'):\n",
        "    try:\n",
        "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Research_Thesis_Bocconi_2023/research-thesis-bocconi-9f42b95f179b.json\"\n",
        "        client = translate.Client()\n",
        "\n",
        "        result = client.translate(text, target_language=target_language)\n",
        "        translation = result['translatedText']\n",
        "        return text, translation\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error for '{text}': {e}\")\n",
        "        return text, None\n",
        "\n",
        "def translate_structure_google_api(structure):\n",
        "    translated_structure = {}\n",
        "    total_words_translated = 0\n",
        "\n",
        "    for key, word_list in structure.items():\n",
        "        translated_words = []\n",
        "        for word in word_list:\n",
        "            original, translation = translate_text_google_api(word)\n",
        "            translated_words.append(translation)\n",
        "            if translation is not None:\n",
        "                total_words_translated += 1\n",
        "                #print(f\"Word '{word}' translated to '{translation}' successfully. Total words translated: {total_words_translated}\")\n",
        "\n",
        "        translated_structure[key] = translated_words\n",
        "\n",
        "    return translated_structure\n",
        "\n",
        "translated_structure = translate_structure_google_api(code_to_lex)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_Eb2uSRf5yE"
      },
      "outputs": [],
      "source": [
        "#shows how many words there are per frame after the translation; should be 250 each\n",
        "\n",
        "#result = {key: len(values) for key, values in translated_structure.items()}\n",
        "\n",
        "#print(\"Number of values per key:\")\n",
        "#print('\\n'.join([f\"{key}: {count}\" for key, count in result.items()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAjlB9aasrcO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/field_2018/code_to_lex_trans_ru.json'\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(translated_structure, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FGpt3FsyJn0"
      },
      "source": [
        "# Translate F_base FR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SL_qHQDyWvA"
      },
      "outputs": [],
      "source": [
        "from google.cloud import translate_v2 as translate\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def translate_text_google_api(text, target_language='fr'):\n",
        "    try:\n",
        "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Research_Thesis_Bocconi_2023/research-thesis-bocconi-9f42b95f179b.json\"\n",
        "        client = translate.Client()\n",
        "\n",
        "        result = client.translate(text, target_language=target_language)\n",
        "        translation = result['translatedText']\n",
        "        return text, translation\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error for '{text}': {e}\")\n",
        "        return text, None\n",
        "\n",
        "def translate_structure_google_api(structure):\n",
        "    translated_structure = {}\n",
        "    total_words_translated = 0\n",
        "\n",
        "    for key, word_list in structure.items():\n",
        "        translated_words = []\n",
        "        for word in word_list:\n",
        "            original, translation = translate_text_google_api(word)\n",
        "            translated_words.append(translation)\n",
        "            if translation is not None:\n",
        "                total_words_translated += 1\n",
        "                #print(f\"Word '{word}' translated to '{translation}' successfully. Total words translated: {total_words_translated}\")\n",
        "\n",
        "        translated_structure[key] = translated_words\n",
        "\n",
        "    return translated_structure\n",
        "\n",
        "translated_structure = translate_structure_google_api(code_to_lex)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0GoBQoQybRO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/field_2018/code_to_lex_trans_fr.json'\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(translated_structure, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bczS69AEyNQn"
      },
      "source": [
        "# Translate F_base IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROvc7hAnyh3h"
      },
      "outputs": [],
      "source": [
        "from google.cloud import translate_v2 as translate\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def translate_text_google_api(text, target_language='it'):\n",
        "    try:\n",
        "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Research_Thesis_Bocconi_2023/research-thesis-bocconi-9f42b95f179b.json\"\n",
        "        client = translate.Client()\n",
        "\n",
        "        result = client.translate(text, target_language=target_language)\n",
        "        translation = result['translatedText']\n",
        "        return text, translation\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error for '{text}': {e}\")\n",
        "        return text, None\n",
        "\n",
        "def translate_structure_google_api(structure):\n",
        "    translated_structure = {}\n",
        "    total_words_translated = 0\n",
        "\n",
        "    for key, word_list in structure.items():\n",
        "        translated_words = []\n",
        "        for word in word_list:\n",
        "            original, translation = translate_text_google_api(word)\n",
        "            translated_words.append(translation)\n",
        "            if translation is not None:\n",
        "                total_words_translated += 1\n",
        "                #print(f\"Word '{word}' translated to '{translation}' successfully. Total words translated: {total_words_translated}\")\n",
        "\n",
        "        translated_structure[key] = translated_words\n",
        "\n",
        "    return translated_structure\n",
        "\n",
        "translated_structure = translate_structure_google_api(code_to_lex)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN5r0Vmjynq-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/field_2018/code_to_lex_trans_it.json'\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(translated_structure, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djF83kFwyRBV"
      },
      "source": [
        "# Translate F_base ES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvLzBTjCyqXK"
      },
      "outputs": [],
      "source": [
        "from google.cloud import translate_v2 as translate\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def translate_text_google_api(text, target_language='es'):\n",
        "    try:\n",
        "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Research_Thesis_Bocconi_2023/research-thesis-bocconi-9f42b95f179b.json\"\n",
        "        client = translate.Client()\n",
        "\n",
        "        result = client.translate(text, target_language=target_language)\n",
        "        translation = result['translatedText']\n",
        "        return text, translation\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error for '{text}': {e}\")\n",
        "        return text, None\n",
        "\n",
        "def translate_structure_google_api(structure):\n",
        "    translated_structure = {}\n",
        "    total_words_translated = 0\n",
        "\n",
        "    for key, word_list in structure.items():\n",
        "        translated_words = []\n",
        "        for word in word_list:\n",
        "            original, translation = translate_text_google_api(word)\n",
        "            translated_words.append(translation)\n",
        "            if translation is not None:\n",
        "                total_words_translated += 1\n",
        "                #print(f\"Word '{word}' translated to '{translation}' successfully. Total words translated: {total_words_translated}\")\n",
        "\n",
        "        translated_structure[key] = translated_words\n",
        "\n",
        "    return translated_structure\n",
        "\n",
        "translated_structure = translate_structure_google_api(code_to_lex)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoKLRSABytR9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/field_2018/code_to_lex_trans_es.json'\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(translated_structure, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orHU3JKLp0Dd"
      },
      "source": [
        "# F_base translated file RU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJmhoTJkw1by"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/field_2018/code_to_lex_trans_ru.json'\n",
        "\n",
        "with open(file_path, 'r') as json_file:\n",
        "    code_to_lex = json.load(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPIlB0ONRC8K"
      },
      "outputs": [],
      "source": [
        "#check that there are actually 250 values per key\n",
        "values_per_key = {key: len(values) for key, values in code_to_lex.items()}\n",
        "\n",
        "for key, count in values_per_key.items():\n",
        "    print(f\"{key}: {count} values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqy8anQNjLET"
      },
      "source": [
        "# Train embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NffzMz4lk0CK"
      },
      "source": [
        "Trying out word2vec on CC-100\n",
        "https://data.statmt.org/cc-100/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kktw8rE5k1kk",
        "outputId": "688c81dc-0c67-4b55-ae77-6cdd76c37b1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-37-789314f19d15>:9: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  ru_background_corpus = pd.read_csv(f)\n"
          ]
        }
      ],
      "source": [
        "# change the training corpus to a relevant one when ready\n",
        "\n",
        "import pandas as pd\n",
        "import bz2\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/lenta-ru-news.csv.bz2'\n",
        "\n",
        "with bz2.open(file_path, 'rt', encoding='utf-8') as f: # decompresses the file\n",
        "    ru_background_corpus = pd.read_csv(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5J4I7VLnBPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02507f90-5ad4-4034-c8de-3798e9a1dc4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f6a9ebe82037>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mru_background_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#len(texts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ru_background_corpus' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "texts = ru_background_corpus['text'].astype(str).tolist()\n",
        "\n",
        "#len(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSy8kTUiNz10"
      },
      "outputs": [],
      "source": [
        "# sample as needed\n",
        "texts = texts[:10000] # execution for 100k time approx. 7 min\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
        "\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "tokenized_texts = [\n",
        "    [word for word in text if word.isalnum() and word not in stop_words]\n",
        "    for text in tokenized_texts\n",
        "]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences=tokenized_texts,\n",
        "    vector_size=200,  # 200 dimensions\n",
        "    window=5, # context window\n",
        "    min_count=1,\n",
        "    workers=5  # speed of training (?)\n",
        ")\n",
        "\n",
        "model.save(\"/content/drive/MyDrive/Research_Thesis_Bocconi_2023/word2vec_model_ru.model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EorVX3YuA3CB"
      },
      "outputs": [],
      "source": [
        "# this function is needed to define a corpus vocabulary\n",
        "\n",
        "def get_article_top_words(input_texts):\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "    c = Counter()\n",
        "    article_counter = Counter()\n",
        "    num_articles = 0\n",
        "\n",
        "    for text in input_texts:\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "        c.update(words)\n",
        "        article_counter.update(set(words))\n",
        "        num_articles += 1\n",
        "\n",
        "    return c, num_articles, article_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwQM_8cAB1nV"
      },
      "outputs": [],
      "source": [
        "#c, num_articles, article_counter = get_article_top_words(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxChzbzGFjJV"
      },
      "outputs": [],
      "source": [
        "#below: how many times a word appeared in all texts, # article total, in how many articles has a word appeared\n",
        "\n",
        "top_words, num_articles, article_counter = get_article_top_words(texts)\n",
        "\n",
        "vocab = sorted(top_words, key=top_words.get, reverse = True)[:50000] # her vocab size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtsJEuseKUGV"
      },
      "outputs": [],
      "source": [
        "##### use this to only filter the F_base_translated to the limits of the background corpus vocab\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def seeds_to_real_lex(raw_lex, model, vocab):\n",
        "    wv_model = Word2Vec.load(model)\n",
        "\n",
        "    # Iterate over words in raw_lex\n",
        "    filtered_seeds = [word for word in raw_lex if word in vocab and word in wv_model.wv]\n",
        "\n",
        "    return filtered_seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo9tgklh8opT"
      },
      "outputs": [],
      "source": [
        "model = \"/content/drive/MyDrive/Research_Thesis_Bocconi_2023/word2vec_model_ru.model\"\n",
        "\n",
        "filtered_code_to_lex = {key: seeds_to_real_lex(value, model, vocab) for key, value in code_to_lex.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6dbzBWDnEK2"
      },
      "outputs": [],
      "source": [
        "code_to_lex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T_SKU-_8Dyz"
      },
      "outputs": [],
      "source": [
        "def cluster_seeds(wv, seeds, topn, threshold, num_clusters=1):\n",
        "    X = [wv[s] for s in seeds]\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(X)\n",
        "    expanded_seeds = []\n",
        "    for center in kmeans.cluster_centers_:\n",
        "        expanded_seeds += [x[0] for x in wv.most_similar(positive=[center], topn=topn) if x[1] >= threshold]\n",
        "        #expanded_seeds += [x for x in seeds if (1 - cosine(center, wv[x])) >= threshold]\n",
        "    return set(expanded_seeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hhcp--oLYAS"
      },
      "outputs": [],
      "source": [
        "# final execution\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Research_Thesis_Bocconi_2023/word2vec_model_ru.model\"\n",
        "wv_model = Word2Vec.load(model_path)\n",
        "\n",
        "expanded_seeds_dict = {}\n",
        "\n",
        "for key in filtered_code_to_lex:\n",
        "    expanded_seeds_dict[key] = cluster_seeds(wv_model.wv, filtered_code_to_lex[key], topn=1000, threshold=0.7, num_clusters=1)\n",
        "\n",
        "# Print the number of elements for each key\n",
        "for key, expanded_seeds in expanded_seeds_dict.items():\n",
        "    print(f\"{key}: {len(expanded_seeds)} elements\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAxfUB8rKoVY"
      },
      "outputs": [],
      "source": [
        "expanded_seeds_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc22BB_hi5t4"
      },
      "source": [
        "# Query expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztcaiCQ0te8i",
        "outputId": "8e551210-49c1-4b40-9996-710b3798cba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13.0: 1000 values\n",
            "6.0: 1000 values\n",
            "1.0: 1000 values\n",
            "14.0: 1000 values\n",
            "12.0: 1000 values\n",
            "2.0: 1000 values\n",
            "11.0: 1000 values\n",
            "9.0: 1000 values\n",
            "3.0: 1000 values\n",
            "10.0: 1000 values\n",
            "5.0: 1000 values\n",
            "8.0: 1000 values\n",
            "4.0: 1000 values\n",
            "7.0: 1000 values\n",
            "15.0: 1000 values\n"
          ]
        }
      ],
      "source": [
        "# just checks how many words per frame you are left with\n",
        "values_per_key = {key: len(values) for key, values in expanded_seeds_dict.items()}\n",
        "\n",
        "\n",
        "for key, count in values_per_key.items():\n",
        "    print(f\"{key}: {count} values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C190mymvRa69"
      },
      "outputs": [],
      "source": [
        "# Again remove the words that appear in 98% and 0,5% of articles\n",
        "\n",
        "cut_words = get_words_to_cut(num_articles, article_counter, 200, 50) # values are adequate for a sample of 10k texts\n",
        "\n",
        "for key, values in expanded_seeds_dict.items():\n",
        "    expanded_seeds_dict[key] = [value for value in values if value not in cut_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSKCvQ4YRQOP"
      },
      "outputs": [],
      "source": [
        "#final counts of F_base_translated_expanded\n",
        "\n",
        "for key, values in expanded_seeds_dict.items():\n",
        "    print(f\"{key}: {len(values)} entries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWsKinv6ueaQ"
      },
      "outputs": [],
      "source": [
        "expanded_seeds_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpKJs-gkuxZV"
      },
      "outputs": [],
      "source": [
        "#saving F_base_translated_expanded\n",
        "\n",
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research_Thesis_Bocconi_2023/expanded_seeds_dict.json'\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(expanded_seeds_dict, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not generate a lexicon for the “Other” frame, and\n",
        "instead assign a document’s primary frame as “Other” only if\n",
        "it does not contain at least 3 words from any framing lexicon.\n",
        "Throughout this process, we use small subsets of the “tobacco”\n",
        "articles for parameter tuning"
      ],
      "metadata": {
        "id": "3jrrtJVLGRJq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNwZw5vaIKck2cyzfDtKopP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}